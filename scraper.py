#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€æ´çš„å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é‡‡é›†å™¨
"""

import requests
import json
import re
import time
import random
from datetime import datetime
from bs4 import BeautifulSoup
from fake_useragent import UserAgent


class WeChatScraper:
    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
        
    def get_headers(self):
        """è·å–éšæœºè¯·æ±‚å¤´"""
        return {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    def clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤æ–‡ç« ç»“å°¾çš„æ¨å¹¿å†…å®¹
        unwanted_patterns = [
            r'è·³è½¬äºŒç»´ç .*',
            r'ä½œè€…å¤´åƒ.*',
            r'é•¿æŒ‰è¯†åˆ«.*',
            r'æ‰«ç å…³æ³¨.*',
            r'ç‚¹å‡»é˜…è¯».*',
        ]
        
        for pattern in unwanted_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def extract_content(self, soup):
        """æå–æ–‡ç« å†…å®¹"""
        # å°è¯•å¤šç§é€‰æ‹©å™¨
        content_selectors = [
            '#js_content',
            '.rich_media_content',
            '.content',
            'article',
            'main'
        ]
        
        content = ""
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                content = element.get_text()
                break
        
        return self.clean_text(content)
    
    def extract_title(self, soup):
        """æå–æ–‡ç« æ ‡é¢˜"""
        title_selectors = [
            'h1#activity-name',
            'h1.rich_media_title',
            'title',
            'h1'
        ]
        
        for selector in title_selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text().strip()
        
        return "æœªçŸ¥æ ‡é¢˜"
    
    def extract_author(self, soup):
        """æå–ä½œè€…ä¿¡æ¯"""
        author_selectors = [
            '.rich_media_meta_text',
            '.author',
            '[data-author]'
        ]
        
        for selector in author_selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text().strip()
        
        return "æœªçŸ¥ä½œè€…"
    
    def scrape_article(self, url):
        """é‡‡é›†æ–‡ç« """
        try:
            print(f"å¼€å§‹é‡‡é›†: {url}")
            
            # éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(1, 3))
            
            # å‘é€è¯·æ±‚
            response = self.session.get(url, headers=self.get_headers(), timeout=30)
            response.raise_for_status()
            
            # è§£æHTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–ä¿¡æ¯
            title = self.extract_title(soup)
            author = self.extract_author(soup)
            content = self.extract_content(soup)
            
            # æ„å»ºç»“æœ
            result = {
                'title': title,
                'author': author,
                'content': content,
                'url': url,
                'scrape_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'word_count': len(content)
            }
            
            print(f"é‡‡é›†æˆåŠŸ: {title}")
            return result
            
        except Exception as e:
            print(f"é‡‡é›†å¤±è´¥: {str(e)}")
            return None
    
    def save_to_file(self, data, filename):
        """ä¿å­˜åˆ°æ–‡ä»¶"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"å·²ä¿å­˜: {filename}")
        except Exception as e:
            print(f"ä¿å­˜å¤±è´¥: {str(e)}")
    
    def generate_html(self, data, recommended_articles=None, url_number=None):
        """ç”ŸæˆHTMLæ–‡ä»¶"""
        # ç”Ÿæˆæ¨èé˜…è¯»åˆ—è¡¨HTML
        recommend_html = ""
        if recommended_articles:
            recommend_html = """
        <div class="recommend-section">
            <h3 class="recommend-title">ğŸ“š æ¨èé˜…è¯»</h3>
            <div class="recommend-list">
"""
            for article in recommended_articles:
                # ä½¿ç”¨æ–°çš„URLæ ¼å¼
                article_url = f"new/{article.get('url_number', '1')}" if article.get('url_number') else article.get('html_file', '#')
                recommend_html += f"""
                <div class="recommend-item">
                    <a href="{article_url}" class="recommend-link">
                        <span class="recommend-title-text">{article['title']}</span>
                        <span class="recommend-meta">{article['author']} Â· {article['scrape_time'][:10]}</span>
                    </a>
                </div>
"""
            recommend_html += """
            </div>
        </div>
"""
        
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{data['title']}</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background: #f8f9fa;
        }}
        .article-container {{
            background: white;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .article-header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .article-title {{
            font-size: 2em;
            margin-bottom: 10px;
            font-weight: 600;
        }}
        .article-meta {{
            opacity: 0.9;
            font-size: 1.1em;
        }}
        .article-content {{
            padding: 40px;
        }}
        .content-text {{
            font-size: 16px;
            line-height: 1.8;
            color: #333;
            white-space: pre-wrap;
        }}
        .recommend-section {{
            background: #f8f9fa;
            padding: 30px;
            border-top: 1px solid #e9ecef;
        }}
        .recommend-title {{
            font-size: 1.3em;
            margin-bottom: 20px;
            color: #333;
            text-align: center;
            font-weight: 600;
        }}
        .recommend-list {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 12px;
        }}
        .recommend-item {{
            background: white;
            border-radius: 8px;
            padding: 15px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
            transition: transform 0.2s, box-shadow 0.2s;
        }}
        .recommend-item:hover {{
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }}
        .recommend-link {{
            text-decoration: none;
            color: inherit;
            display: block;
        }}
        .recommend-title-text {{
            font-size: 14px;
            font-weight: 600;
            color: #333;
            line-height: 1.4;
            display: block;
            margin-bottom: 6px;
        }}
        .recommend-meta {{
            font-size: 12px;
            color: #666;
            display: block;
        }}
        .article-footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #666;
            border-top: 1px solid #e9ecef;
        }}
        .word-count {{
            font-size: 14px;
            margin-top: 10px;
        }}
        @media (max-width: 768px) {{
            .recommend-list {{
                grid-template-columns: 1fr;
            }}
            .article-content {{
                padding: 20px;
            }}
            .recommend-section {{
                padding: 20px;
            }}
        }}
    </style>
</head>
<body>
    <div class="article-container">
        <div class="article-header">
            <h1 class="article-title">{data['title']}</h1>
            <div class="article-meta">
                <div>ä½œè€…: {data['author']}</div>
                <div>æ—¶é—´: {data['scrape_time']}</div>
            </div>
        </div>
        
        <div class="article-content">
            <div class="content-text">{data['content']}</div>
        </div>
        
        {recommend_html}
        
        <div class="article-footer">
            <div>é‡‡é›†æ—¶é—´: {data['scrape_time']}</div>
            <div class="word-count">å­—æ•°: {data['word_count']}</div>
        </div>
    </div>
</body>
</html>
"""
        return html_content
    
    def save_html(self, data, filename, db=None, url_number=None):
        """ä¿å­˜HTMLæ–‡ä»¶"""
        try:
            # è·å–æ¨èæ–‡ç« 
            recommended_articles = None
            if db:
                try:
                    recommended_articles = db.get_random_articles(15)
                    # è¿‡æ»¤æ‰å½“å‰æ–‡ç« ï¼ˆä½¿ç”¨URLç¼–å·ï¼‰
                    if url_number:
                        recommended_articles = [article for article in recommended_articles if article.get('url_number') != url_number]
                    else:
                        # å¦‚æœæ²¡æœ‰URLç¼–å·ï¼Œä½¿ç”¨æ ‡é¢˜è¿‡æ»¤
                        recommended_articles = [article for article in recommended_articles if article['title'] != data['title']]
                except Exception as e:
                    print(f"è·å–æ¨èæ–‡ç« å¤±è´¥: {str(e)}")
            
            html_content = self.generate_html(data, recommended_articles, url_number)
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(html_content)
            print(f"å·²ä¿å­˜HTML: {filename}")
        except Exception as e:
            print(f"ä¿å­˜HTMLå¤±è´¥: {str(e)}")


def main():
    """ä¸»å‡½æ•°"""
    scraper = WeChatScraper()
    
    # æµ‹è¯•URL
    test_url = "https://mp.weixin.qq.com/s?src=11&timestamp=1757648270&ver=6231&signature=joUe7cttth4-xe6-Tzaib4kd8AZOyO2SWEvBfhjAlCKJ80XR55A7dGf83gjGF603c40uL7-eWba50YV8*cAfYbhtpGTiLo20i8AfOjcYhCwaX7JtET2xg5RGQ*WMMFtZ&new=1"
    
    # é‡‡é›†æ–‡ç« 
    result = scraper.scrape_article(test_url)
    
    if result:
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        json_filename = f"article_{timestamp}.json"
        html_filename = f"article_{timestamp}.html"
        
        scraper.save_to_file(result, json_filename)
        scraper.save_html(result, html_filename)
        
        # æ‰“å°ç»“æœ
        print("\n=== é‡‡é›†ç»“æœ ===")
        print(f"æ ‡é¢˜: {result['title']}")
        print(f"ä½œè€…: {result['author']}")
        print(f"å­—æ•°: {result['word_count']}")
        print(f"æ—¶é—´: {result['scrape_time']}")
        print(f"JSONæ–‡ä»¶: {json_filename}")
        print(f"HTMLæ–‡ä»¶: {html_filename}")
    else:
        print("é‡‡é›†å¤±è´¥")


if __name__ == "__main__":
    main()

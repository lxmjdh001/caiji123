#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨é‡‡é›†ç³»ç»Ÿ
"""

import requests
import json
import re
import time
import random
import threading
from datetime import datetime
from typing import List, Dict
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from database import Database
from scraper import WeChatScraper

class AutoScraper:
    def __init__(self, db_path: str = "articles.db"):
        self.db = Database(db_path)
        self.scraper = WeChatScraper()
        self.ua = UserAgent()
        self.session = requests.Session()
        self.is_running = False
        
    def get_headers(self):
        """è·å–éšæœºè¯·æ±‚å¤´"""
        return {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    def search_wechat_articles(self, keyword: str, max_pages: int = 3) -> List[str]:
        """æœç´¢å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥"""
        print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")
        
        # è¿™é‡Œä½¿ç”¨æœç‹—å¾®ä¿¡æœç´¢
        search_urls = [
            f"https://weixin.sogou.com/weixin?type=2&query={keyword}&ie=utf8",
            f"https://weixin.sogou.com/weixin?type=2&query={keyword}&ie=utf8&page=2",
            f"https://weixin.sogou.com/weixin?type=2&query={keyword}&ie=utf8&page=3"
        ]
        
        article_urls = []
        
        for url in search_urls[:max_pages]:
            try:
                print(f"ğŸ“„ æœç´¢é¡µé¢: {url}")
                time.sleep(random.uniform(2, 4))  # éšæœºå»¶è¿Ÿ
                
                response = self.session.get(url, headers=self.get_headers(), timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾æ–‡ç« é“¾æ¥
                links = soup.find_all('a', href=True)
                for link in links:
                    href = link.get('href', '')
                    if 'mp.weixin.qq.com' in href and '/s?' in href:
                        # æ¸…ç†é“¾æ¥
                        clean_url = href.split('&')[0] + '&' + '&'.join(href.split('&')[1:])
                        if clean_url not in article_urls:
                            article_urls.append(clean_url)
                
                print(f"âœ… æ‰¾åˆ° {len(article_urls)} ä¸ªæ–‡ç« é“¾æ¥")
                
            except Exception as e:
                print(f"âŒ æœç´¢å¤±è´¥: {str(e)}")
                continue
        
        return article_urls
    
    def auto_scrape_keyword(self, keyword: str, max_articles: int = 10) -> Dict:
        """è‡ªåŠ¨é‡‡é›†æŒ‡å®šå…³é”®è¯çš„æ–‡ç« """
        print(f"ğŸš€ å¼€å§‹è‡ªåŠ¨é‡‡é›†å…³é”®è¯: {keyword}")
        
        # æ·»åŠ å…³é”®è¯åˆ°æ•°æ®åº“
        keyword_id = self.db.add_keyword(keyword)
        
        # åˆ›å»ºä»»åŠ¡
        task_id = self.db.add_task(keyword_id, 'auto_scrape')
        self.db.update_task_status(task_id, 'running')
        
        try:
            # æœç´¢æ–‡ç« é“¾æ¥
            article_urls = self.search_wechat_articles(keyword, max_pages=3)
            
            if not article_urls:
                self.db.update_task_status(task_id, 'failed', error='æœªæ‰¾åˆ°æ–‡ç« é“¾æ¥')
                return {'success': False, 'message': 'æœªæ‰¾åˆ°æ–‡ç« é“¾æ¥'}
            
            # é™åˆ¶é‡‡é›†æ•°é‡
            article_urls = article_urls[:max_articles]
            
            scraped_count = 0
            failed_count = 0
            
            for i, url in enumerate(article_urls, 1):
                try:
                    print(f"ğŸ“° é‡‡é›†ç¬¬ {i}/{len(article_urls)} ç¯‡æ–‡ç« ")
                    
                    # é‡‡é›†æ–‡ç« 
                    result = self.scraper.scrape_article(url)
                    
                    if result:
                        # è·å–ä¸‹ä¸€ä¸ªURLç¼–å·
                        url_number = self.db.get_next_url_number()
                        
                        # ç”ŸæˆHTMLæ–‡ä»¶å
                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                        safe_title = re.sub(r'[^\w\s-]', '', result['title'])[:50]
                        html_filename = f"{safe_title}_{timestamp}.html"
                        
                        result['html_file'] = html_filename
                        result['url_number'] = url_number
                        
                        # ä¿å­˜åˆ°æ•°æ®åº“
                        self.db.add_article(result, keyword_id)
                        
                        # ç”ŸæˆHTMLæ–‡ä»¶
                        self.scraper.save_html(result, html_filename, self.db, url_number)
                        scraped_count += 1
                        
                        print(f"âœ… é‡‡é›†æˆåŠŸ: {result['title']}")
                    else:
                        failed_count += 1
                        print(f"âŒ é‡‡é›†å¤±è´¥: {url}")
                    
                    # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°
                    time.sleep(random.uniform(3, 6))
                    
                except Exception as e:
                    failed_count += 1
                    print(f"âŒ é‡‡é›†å¼‚å¸¸: {str(e)}")
                    continue
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            result_data = {
                'scraped_count': scraped_count,
                'failed_count': failed_count,
                'total_urls': len(article_urls)
            }
            
            self.db.update_task_status(task_id, 'completed', json.dumps(result_data))
            
            return {
                'success': True,
                'message': f'é‡‡é›†å®Œæˆ: æˆåŠŸ {scraped_count} ç¯‡ï¼Œå¤±è´¥ {failed_count} ç¯‡',
                'data': result_data
            }
            
        except Exception as e:
            self.db.update_task_status(task_id, 'failed', error=str(e))
            return {'success': False, 'message': str(e)}
    
    def batch_scrape_keywords(self, keywords: List[str], max_articles_per_keyword: int = 5):
        """æ‰¹é‡é‡‡é›†å¤šä¸ªå…³é”®è¯"""
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡é‡‡é›† {len(keywords)} ä¸ªå…³é”®è¯")
        
        results = []
        for i, keyword in enumerate(keywords, 1):
            print(f"\nğŸ“ å¤„ç†å…³é”®è¯ {i}/{len(keywords)}: {keyword}")
            
            result = self.auto_scrape_keyword(keyword, max_articles_per_keyword)
            results.append({
                'keyword': keyword,
                'result': result
            })
            
            # å…³é”®è¯é—´å»¶è¿Ÿ
            if i < len(keywords):
                delay = random.uniform(10, 20)
                print(f"â³ ç­‰å¾… {delay:.1f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªå…³é”®è¯...")
                time.sleep(delay)
        
        return results
    
    def start_auto_scraper(self, keywords: List[str], interval_hours: int = 6, max_articles_per_keyword: int = 5):
        """å¯åŠ¨è‡ªåŠ¨é‡‡é›†æœåŠ¡"""
        if self.is_running:
            return {'success': False, 'message': 'è‡ªåŠ¨é‡‡é›†æœåŠ¡å·²åœ¨è¿è¡Œ'}
        
        self.is_running = True
        
        def auto_scrape_loop():
            while self.is_running:
                try:
                    print(f"\nğŸ”„ å¼€å§‹æ–°ä¸€è½®è‡ªåŠ¨é‡‡é›† - {datetime.now()}")
                    
                    # æ‰¹é‡é‡‡é›†
                    results = self.batch_scrape_keywords(keywords, max_articles_per_keyword)
                    
                    # ç»Ÿè®¡ç»“æœ
                    total_success = sum(1 for r in results if r['result']['success'])
                    total_failed = len(results) - total_success
                    
                    print(f"\nğŸ“Š æœ¬è½®é‡‡é›†å®Œæˆ: æˆåŠŸ {total_success} ä¸ªå…³é”®è¯ï¼Œå¤±è´¥ {total_failed} ä¸ª")
                    
                    # ç­‰å¾…ä¸‹æ¬¡é‡‡é›†
                    if self.is_running:
                        wait_seconds = interval_hours * 3600
                        print(f"â° ç­‰å¾… {interval_hours} å°æ—¶åè¿›è¡Œä¸‹ä¸€è½®é‡‡é›†...")
                        time.sleep(wait_seconds)
                    
                except Exception as e:
                    print(f"âŒ è‡ªåŠ¨é‡‡é›†å¼‚å¸¸: {str(e)}")
                    time.sleep(300)  # å¼‚å¸¸åç­‰å¾…5åˆ†é’Ÿ
        
        # å¯åŠ¨åå°çº¿ç¨‹
        thread = threading.Thread(target=auto_scrape_loop)
        thread.daemon = True
        thread.start()
        
        return {'success': True, 'message': f'è‡ªåŠ¨é‡‡é›†æœåŠ¡å·²å¯åŠ¨ï¼Œæ¯ {interval_hours} å°æ—¶é‡‡é›†ä¸€æ¬¡'}
    
    def stop_auto_scraper(self):
        """åœæ­¢è‡ªåŠ¨é‡‡é›†æœåŠ¡"""
        self.is_running = False
        return {'success': True, 'message': 'è‡ªåŠ¨é‡‡é›†æœåŠ¡å·²åœæ­¢'}
    
    def get_scraping_status(self):
        """è·å–é‡‡é›†çŠ¶æ€"""
        stats = self.db.get_stats()
        recent_tasks = self.db.get_tasks(limit=10)
        
        return {
            'is_running': self.is_running,
            'stats': stats,
            'recent_tasks': recent_tasks
        }


def main():
    """æµ‹è¯•è‡ªåŠ¨é‡‡é›†åŠŸèƒ½"""
    auto_scraper = AutoScraper()
    
    # æµ‹è¯•å…³é”®è¯
    test_keywords = ['åŠ æ‹¿å¤§ç§»æ°‘', 'ç•™å­¦']
    
    print("ğŸ§ª æµ‹è¯•è‡ªåŠ¨é‡‡é›†åŠŸèƒ½")
    print("=" * 50)
    
    # æµ‹è¯•å•ä¸ªå…³é”®è¯é‡‡é›†
    result = auto_scraper.auto_scrape_keyword(test_keywords[0], max_articles=3)
    print(f"é‡‡é›†ç»“æœ: {result}")
    
    # æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
    stats = auto_scraper.get_scraping_status()
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}")


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨é‡‡é›†ç³»ç»Ÿ
"""

import requests
import json
import re
import time
import random
import threading
from datetime import datetime
from typing import List, Dict
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from database import Database
from scraper import WeChatScraper

class AutoScraper:
    def __init__(self, db_path: str = "articles.db"):
        self.db = Database(db_path)
        self.scraper = WeChatScraper()
        self.ua = UserAgent()
        self.session = requests.Session()
        self.is_running = False
        self.batch_size = 100  # æ¯æ‰¹é‡‡é›†100ç¯‡
        self.rest_minutes = 5   # ä¼‘æ¯5åˆ†é’Ÿ
        self.current_batch_count = 0  # å½“å‰æ‰¹æ¬¡è®¡æ•°
        
    def get_headers(self):
        """è·å–éšæœºè¯·æ±‚å¤´"""
        return {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    def search_wechat_articles(self, keyword: str, max_pages: int = 3) -> List[str]:
        """æœç´¢å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥"""
        print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")
        
        # ä½¿ç”¨å¤šä¸ªæœç´¢æº
        search_sources = [
            # æœç‹—å¾®ä¿¡æœç´¢
            f"https://weixin.sogou.com/weixin?type=2&query={keyword}&ie=utf8",
            f"https://weixin.sogou.com/weixin?type=2&query={keyword}&ie=utf8&page=2",
            f"https://weixin.sogou.com/weixin?type=2&query={keyword}&ie=utf8&page=3",
            # ç™¾åº¦æœç´¢å¾®ä¿¡æ–‡ç« 
            f"https://www.baidu.com/s?wd=site:mp.weixin.qq.com {keyword}",
            # Googleæœç´¢ï¼ˆå¦‚æœå¯è®¿é—®ï¼‰
            f"https://www.google.com/search?q=site:mp.weixin.qq.com {keyword}"
        ]
        
        article_urls = []
        
        for i, url in enumerate(search_sources[:max_pages]):
            try:
                print(f"ğŸ“„ æœç´¢é¡µé¢ {i+1}: {url}")
                time.sleep(random.uniform(3, 6))  # å¢åŠ å»¶è¿Ÿ
                
                # å¢å¼ºè¯·æ±‚å¤´
                headers = self.get_headers()
                headers.update({
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none',
                    'Upgrade-Insecure-Requests': '1'
                })
                
                response = self.session.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # å¤šç§æ–¹å¼æŸ¥æ‰¾æ–‡ç« é“¾æ¥
                found_links = set()
                
                # æ–¹æ³•1: æŸ¥æ‰¾æœç‹—æœç´¢ç»“æœä¸­çš„é“¾æ¥
                links = soup.find_all('a', href=True)
                for link in links:
                    href = link.get('href', '')
                    
                    # å¤„ç†æœç‹—çš„é‡å®šå‘é“¾æ¥
                    if 'weixin.sogou.com' in href and 'url=' in href:
                        import urllib.parse
                        try:
                            # æå–é‡å®šå‘çš„çœŸå®URL
                            parsed = urllib.parse.parse_qs(urllib.parse.urlparse(href).query)
                            if 'url' in parsed and parsed['url']:
                                real_url = parsed['url'][0]
                                # URLè§£ç 
                                real_url = urllib.parse.unquote(real_url)
                                if 'mp.weixin.qq.com' in real_url and '/s?' in real_url:
                                    if real_url not in found_links:
                                        found_links.add(real_url)
                                        article_urls.append(real_url)
                                        print(f"ğŸ”— æ‰¾åˆ°æ–‡ç« é“¾æ¥: {real_url}")
                        except Exception as e:
                            print(f"è§£æé“¾æ¥å¤±è´¥: {e}")
                            continue
                    
                    # ç›´æ¥åŒ…å«å¾®ä¿¡é“¾æ¥çš„æƒ…å†µ
                    elif 'mp.weixin.qq.com' in href and '/s?' in href:
                        if href.startswith('//'):
                            href = 'https:' + href
                        elif href.startswith('/'):
                            href = 'https://mp.weixin.qq.com' + href
                        
                        if href not in found_links:
                            found_links.add(href)
                            article_urls.append(href)
                            print(f"ğŸ”— æ‰¾åˆ°ç›´æ¥é“¾æ¥: {href}")
                
                # æ–¹æ³•2: æŸ¥æ‰¾æœç´¢ç»“æœåŒºåŸŸçš„é“¾æ¥
                result_divs = soup.find_all('div', class_=['result', 'news-item', 'txt-box'])
                for div in result_divs:
                    link = div.find('a', href=True)
                    if link:
                        href = link.get('href', '')
                        if 'weixin.sogou.com' in href and 'url=' in href:
                            import urllib.parse
                            try:
                                parsed = urllib.parse.parse_qs(urllib.parse.urlparse(href).query)
                                if 'url' in parsed and parsed['url']:
                                    real_url = urllib.parse.unquote(parsed['url'][0])
                                    if 'mp.weixin.qq.com' in real_url:
                                        if real_url not in found_links:
                                            found_links.add(real_url)
                                            article_urls.append(real_url)
                                            print(f"ğŸ”— æ‰¾åˆ°ç»“æœé“¾æ¥: {real_url}")
                            except Exception as e:
                                continue
                
                print(f"âœ… æ‰¾åˆ° {len(found_links)} ä¸ªæ–°æ–‡ç« é“¾æ¥")
                
            except Exception as e:
                print(f"âŒ æœç´¢å¤±è´¥: {str(e)}")
                continue
        
        # å»é‡å¹¶è¿”å›
        unique_urls = list(dict.fromkeys(article_urls))  # ä¿æŒé¡ºåºçš„å»é‡
        print(f"ğŸ¯ æ€»å…±æ‰¾åˆ° {len(unique_urls)} ä¸ªå”¯ä¸€æ–‡ç« é“¾æ¥")
        
        return unique_urls
    
    def auto_scrape_keyword(self, keyword: str, max_articles: int = 10) -> Dict:
        """è‡ªåŠ¨é‡‡é›†æŒ‡å®šå…³é”®è¯çš„æ–‡ç« """
        print(f"ğŸš€ å¼€å§‹è‡ªåŠ¨é‡‡é›†å…³é”®è¯: {keyword}")
        
        # æ·»åŠ å…³é”®è¯åˆ°æ•°æ®åº“
        keyword_id = self.db.add_keyword(keyword)
        
        # åˆ›å»ºä»»åŠ¡
        task_id = self.db.add_task(keyword_id, 'auto_scrape')
        self.db.update_task_status(task_id, 'running')
        
        try:
            # æœç´¢æ–‡ç« é“¾æ¥
            article_urls = self.search_wechat_articles(keyword, max_pages=3)
            
            if not article_urls:
                self.db.update_task_status(task_id, 'failed', error='æœªæ‰¾åˆ°æ–‡ç« é“¾æ¥')
                return {'success': False, 'message': 'æœªæ‰¾åˆ°æ–‡ç« é“¾æ¥'}
            
            # é™åˆ¶é‡‡é›†æ•°é‡
            article_urls = article_urls[:max_articles]
            
            scraped_count = 0
            failed_count = 0
            
            for i, url in enumerate(article_urls, 1):
                try:
                    print(f"ğŸ“° é‡‡é›†ç¬¬ {i}/{len(article_urls)} ç¯‡æ–‡ç« ")
                    
                    # é‡‡é›†æ–‡ç« 
                    result = self.scraper.scrape_article(url)
                    
                    if result:
                        # è·å–ä¸‹ä¸€ä¸ªURLç¼–å·
                        url_number = self.db.get_next_url_number()
                        
                        # ç”ŸæˆHTMLæ–‡ä»¶å
                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                        safe_title = re.sub(r'[^\w\s-]', '', result['title'])[:50]
                        html_filename = f"{safe_title}_{timestamp}.html"
                        
                        result['html_file'] = html_filename
                        result['url_number'] = url_number
                        
                        # ä¿å­˜åˆ°æ•°æ®åº“
                        self.db.add_article(result, keyword_id)
                        
                        # ç”ŸæˆHTMLæ–‡ä»¶
                        self.scraper.save_html(result, html_filename, self.db, url_number)
                        scraped_count += 1
                        self.current_batch_count += 1
                        
                        print(f"âœ… é‡‡é›†æˆåŠŸ: {result['title']} (ç´¯è®¡: {self.current_batch_count})")
                        
                        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹é‡ä¼‘æ¯æ¡ä»¶
                        if self.current_batch_count >= self.batch_size:
                            print(f"ğŸ›Œ å·²é‡‡é›† {self.current_batch_count} ç¯‡æ–‡ç« ï¼Œä¼‘æ¯ {self.rest_minutes} åˆ†é’Ÿ...")
                            time.sleep(self.rest_minutes * 60)  # ä¼‘æ¯5åˆ†é’Ÿ
                            self.current_batch_count = 0  # é‡ç½®è®¡æ•°
                            print("â° ä¼‘æ¯ç»“æŸï¼Œç»§ç»­é‡‡é›†...")
                    else:
                        failed_count += 1
                        print(f"âŒ é‡‡é›†å¤±è´¥: {url}")
                    
                    # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°
                    time.sleep(random.uniform(3, 6))
                    
                except Exception as e:
                    failed_count += 1
                    print(f"âŒ é‡‡é›†å¼‚å¸¸: {str(e)}")
                    continue
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            result_data = {
                'scraped_count': scraped_count,
                'failed_count': failed_count,
                'total_urls': len(article_urls)
            }
            
            self.db.update_task_status(task_id, 'completed', json.dumps(result_data))
            
            return {
                'success': True,
                'message': f'é‡‡é›†å®Œæˆ: æˆåŠŸ {scraped_count} ç¯‡ï¼Œå¤±è´¥ {failed_count} ç¯‡',
                'data': result_data
            }
            
        except Exception as e:
            self.db.update_task_status(task_id, 'failed', error=str(e))
            return {'success': False, 'message': str(e)}
    
    def batch_scrape_keywords(self, keywords: List[str], max_articles_per_keyword: int = 5):
        """æ‰¹é‡é‡‡é›†å¤šä¸ªå…³é”®è¯"""
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡é‡‡é›† {len(keywords)} ä¸ªå…³é”®è¯")
        
        results = []
        for i, keyword in enumerate(keywords, 1):
            print(f"\nğŸ“ å¤„ç†å…³é”®è¯ {i}/{len(keywords)}: {keyword}")
            
            result = self.auto_scrape_keyword(keyword, max_articles_per_keyword)
            results.append({
                'keyword': keyword,
                'result': result
            })
            
            # å…³é”®è¯é—´å»¶è¿Ÿ
            if i < len(keywords):
                delay = random.uniform(10, 20)
                print(f"â³ ç­‰å¾… {delay:.1f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªå…³é”®è¯...")
                time.sleep(delay)
        
        return results
    
    def start_auto_scraper(self, keywords: List[str], interval_hours: int = 6, max_articles_per_keyword: int = 5):
        """å¯åŠ¨è‡ªåŠ¨é‡‡é›†æœåŠ¡"""
        if self.is_running:
            return {'success': False, 'message': 'è‡ªåŠ¨é‡‡é›†æœåŠ¡å·²åœ¨è¿è¡Œ'}
        
        self.is_running = True
        
        def auto_scrape_loop():
            while self.is_running:
                try:
                    print(f"\nğŸ”„ å¼€å§‹æ–°ä¸€è½®è‡ªåŠ¨é‡‡é›† - {datetime.now()}")
                    
                    # æ‰¹é‡é‡‡é›†
                    results = self.batch_scrape_keywords(keywords, max_articles_per_keyword)
                    
                    # ç»Ÿè®¡ç»“æœ
                    total_success = sum(1 for r in results if r['result']['success'])
                    total_failed = len(results) - total_success
                    
                    print(f"\nğŸ“Š æœ¬è½®é‡‡é›†å®Œæˆ: æˆåŠŸ {total_success} ä¸ªå…³é”®è¯ï¼Œå¤±è´¥ {total_failed} ä¸ª")
                    
                    # ç­‰å¾…ä¸‹æ¬¡é‡‡é›†
                    if self.is_running:
                        wait_seconds = interval_hours * 3600
                        print(f"â° ç­‰å¾… {interval_hours} å°æ—¶åè¿›è¡Œä¸‹ä¸€è½®é‡‡é›†...")
                        time.sleep(wait_seconds)
                    
                except Exception as e:
                    print(f"âŒ è‡ªåŠ¨é‡‡é›†å¼‚å¸¸: {str(e)}")
                    time.sleep(300)  # å¼‚å¸¸åç­‰å¾…5åˆ†é’Ÿ
        
        # å¯åŠ¨åå°çº¿ç¨‹
        thread = threading.Thread(target=auto_scrape_loop)
        thread.daemon = True
        thread.start()
        
        return {'success': True, 'message': f'è‡ªåŠ¨é‡‡é›†æœåŠ¡å·²å¯åŠ¨ï¼Œæ¯ {interval_hours} å°æ—¶é‡‡é›†ä¸€æ¬¡'}
    
    def stop_auto_scraper(self):
        """åœæ­¢è‡ªåŠ¨é‡‡é›†æœåŠ¡"""
        self.is_running = False
        return {'success': True, 'message': 'è‡ªåŠ¨é‡‡é›†æœåŠ¡å·²åœæ­¢'}
    
    def set_batch_settings(self, batch_size: int = 100, rest_minutes: int = 5):
        """è®¾ç½®æ‰¹é‡é‡‡é›†å‚æ•°"""
        self.batch_size = batch_size
        self.rest_minutes = rest_minutes
        self.current_batch_count = 0  # é‡ç½®è®¡æ•°
        print(f"ğŸ“Š æ‰¹é‡é‡‡é›†è®¾ç½®å·²æ›´æ–°: æ¯ {batch_size} ç¯‡ä¼‘æ¯ {rest_minutes} åˆ†é’Ÿ")
    
    def get_scraping_status(self):
        """è·å–é‡‡é›†çŠ¶æ€"""
        stats = self.db.get_stats()
        recent_tasks = self.db.get_tasks(limit=10)
        
        return {
            'is_running': self.is_running,
            'stats': stats,
            'recent_tasks': recent_tasks,
            'batch_settings': {
                'batch_size': self.batch_size,
                'rest_minutes': self.rest_minutes,
                'current_count': self.current_batch_count
            }
        }


def main():
    """æµ‹è¯•è‡ªåŠ¨é‡‡é›†åŠŸèƒ½"""
    auto_scraper = AutoScraper()
    
    # æµ‹è¯•å…³é”®è¯
    test_keywords = ['åŠ æ‹¿å¤§ç§»æ°‘', 'ç•™å­¦']
    
    print("ğŸ§ª æµ‹è¯•è‡ªåŠ¨é‡‡é›†åŠŸèƒ½")
    print("=" * 50)
    
    # æµ‹è¯•å•ä¸ªå…³é”®è¯é‡‡é›†
    result = auto_scraper.auto_scrape_keyword(test_keywords[0], max_articles=3)
    print(f"é‡‡é›†ç»“æœ: {result}")
    
    # æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
    stats = auto_scraper.get_scraping_status()
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}")


if __name__ == "__main__":
    main()
